#!/usr/bin/env python
import os
import sqlite3
import re
import numpy as np

from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from transformers import pipeline

DB_PATH     = 'db/emails.db'
EMBED_MODEL = 'all-MiniLM-L6-v2'
LEX_LIMIT   = 10
SEM_LIMIT   = 10

# Preload models
_embedder   = SentenceTransformer(EMBED_MODEL)
_classifier = pipeline(
    "zero-shot-classification",
    model="facebook/bart-large-mnli"
)

# Your human-readable categories
LABELS = [
    "legal",
    "financial",
    "project discussion",
    "human resources",
    "operations",
    "general"
]

# In-memory caches for the entire corpus
_ALL = {
    'ids': None,
    # 'senders': None,
    # 'receivers': None,
    'subjects': None,
    'bodies': None,
    'embeddings': None
}

def _fast_summarize(text, max_sentences=3):
    """Simple extractive: take the first N sentences."""
    sentences = re.split(r'(?<=[\.!?])\s+', text.strip())
    return ' '.join(sentences[:max_sentences])

def _ensure_fts_index():
    """
    Rebuild an FTS4 index that covers subject, body, sender, and receiver.
    """
    conn = sqlite3.connect(DB_PATH)
    cur  = conn.cursor()

    cur.execute("DROP TABLE IF EXISTS email_fts;")
    cur.execute("""
      CREATE VIRTUAL TABLE email_fts
      USING fts4(
        subject,
        body,
        content='emails'
      );
    """)
    # cur.execute("""
    #   CREATE VIRTUAL TABLE email_fts
    #   USING fts4(
    #     subject,
    #     body,
    #     sender,
    #     receiver,
    #     content='emails'
    #   );
    # """)
    cur.execute("""
      INSERT INTO email_fts(rowid, subject, body)
      SELECT id, subject, body
        FROM emails;
    """)
    # cur.execute("""
    #   INSERT INTO email_fts(rowid, subject, body, sender, receiver)
    #   SELECT id, subject, body, sender, receiver
    #     FROM emails;
    # """)
    conn.commit()
    conn.close()

def _load_corpus_embeddings():
    """
    Load id, sender, receiver, subject, body for all emails once,
    then compute their embeddings.
    """
    if _ALL['ids'] is not None:
        return

    conn           = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    cur            = conn.cursor()
    cur.execute("SELECT id, subject, body FROM emails;")
    # cur.execute("SELECT id, sender, receiver, subject, body FROM emails;")
    rows           = cur.fetchall()
    conn.close()

    _ALL['ids']       = [r['id']       for r in rows]
    # _ALL['senders']   = [r['sender']   for r in rows]
    # _ALL['receivers'] = [r['receiver'] for r in rows]
    _ALL['subjects']  = [r['subject']  for r in rows]
    _ALL['bodies']    = [r['body']     for r in rows]

    print(f"Embedding {_ALL['ids'].__len__()} emailsâ€¦")
    _ALL['embeddings'] = _embedder.encode(_ALL['bodies'], show_progress_bar=True)

def _fts_search(query):
    """
    Lexical search (AND between terms) over subject, body, sender, receiver.
    """
    terms = [t for t in query.strip().split() if t]
    fts_q = ' AND '.join(terms) if len(terms) > 1 else terms[0]

    conn           = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    cur            = conn.cursor()
    cur.execute("""
      SELECT e.id, e.subject, e.body
        FROM email_fts
        JOIN emails e ON e.id = email_fts.rowid
       WHERE email_fts MATCH ?
       LIMIT ?
    """, (fts_q, LEX_LIMIT))
    # cur.execute("""
    #   SELECT e.id, e.sender, e.receiver, e.subject, e.body
    #     FROM email_fts
    #     JOIN emails e ON e.id = email_fts.rowid
    #    WHERE email_fts MATCH ?
    #    LIMIT ?
    # """, (fts_q, LEX_LIMIT))
    rows = cur.fetchall()
    conn.close()

    return [dict(r) for r in rows]

def _semantic_search(query):
    """
    Semantic search via cosine similarity over precomputed embeddings.
    """
    _load_corpus_embeddings()

    qv   = _embedder.encode([query])[0].astype('float32')
    mats = np.dot(_ALL['embeddings'], qv) / (
        np.linalg.norm(_ALL['embeddings'], axis=1) * np.linalg.norm(qv)
    )
    top_idx = np.argsort(mats)[::-1][:SEM_LIMIT]

    results = []
    for i in top_idx:
        results.append({
            'id':        _ALL['ids'][i],
            # 'sender':    _ALL['senders'][i],
            # 'receiver':  _ALL['receivers'][i],
            'subject':   _ALL['subjects'][i],
            'body':      _ALL['bodies'][i],
            'sim_score': float(mats[i])
        })
    return results

def _summarize_and_categorize(results):
    """
    1) Extractive summary
    2) Tiny K-Means into up to 3 clusters
    3) Zero-shot classification into LABELS
    """
    # 1) Summaries
    for r in results:
        r['summary'] = _fast_summarize(r.get('body', ''))

    # 2) Clustering
    texts = [r['summary'] for r in results]
    if len(texts) > 1:
        embs    = _embedder.encode(texts)
        k       = min(len(texts), 3)
        km      = KMeans(n_clusters=k, random_state=0).fit(embs)
        labels  = km.labels_
    else:
        labels = [0] * len(texts)

    for r, lab in zip(results, labels):
        r['category'] = int(lab)

    # 3) Zero-shot classification
    for r in results:
        text = r.get('body') or r['summary']
        out  = _classifier(text, LABELS)
        r['classification'] = out['labels'][0]

    return results

def search_emails(query):
    """
    Public API: returns a dict with 'lexical' and 'semantic' lists,
    each enriched on-demand with summary, category, classification.
    """
    if not os.path.exists(DB_PATH):
        raise FileNotFoundError("No db/emails.db found; run ingestion first")

    # (Re)build FTS index
    _ensure_fts_index()

    # Run both searches
    lex = _fts_search(query)
    sem = _semantic_search(query)

    # Enrich only these small sets
    lex = _summarize_and_categorize(lex)
    sem = _summarize_and_categorize(sem)

    return {'lexical': lex, 'semantic': sem}
